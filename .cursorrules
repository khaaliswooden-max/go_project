# GoLlama Runtime (GLR) - Cursor AI Rules
# This file helps Cursor understand the project structure and coding conventions.

## Project Overview
GoLlama Runtime (GLR) is a production-grade LLM tool orchestration runtime built in Go.
It's designed as a learning project that teaches systems programming from undergraduate
to postgraduate level through hands-on implementation.

## Project Structure
```
go_project/
├── cmd/glr/              # Application entry point (minimal, just configuration)
├── internal/             # Private packages (cannot be imported externally)
│   ├── ollama/           # Ollama API client with interface-based design
│   ├── server/           # HTTP server with graceful shutdown
│   ├── audit/            # Structured audit logging
│   └── middleware/       # HTTP middleware (correlation IDs, logging)
├── pkg/                  # Public packages (can be imported)
│   ├── types/            # Shared domain types
│   └── errors/           # Sentinel errors with error codes
├── docs/                 # Documentation
└── .github/workflows/    # CI pipeline
```

## Go Coding Conventions

### Interfaces
- Define interfaces at the point of USE, not implementation
- Keep interfaces small and focused (Interface Segregation Principle)
- Return interfaces from constructors, not concrete types
- Example: `func New(cfg Config) (Client, error)` ✓

### Error Handling
- Use sentinel errors for programmatic checking: `var ErrNotFound = errors.New("not found")`
- Wrap errors with context: `fmt.Errorf("operation failed: %w", err)`
- Check wrapped errors with: `errors.Is(err, ErrNotFound)`
- Never ignore errors; if intentional, use `_ = err` with comment

### Context
- Always pass `context.Context` as the first parameter
- Use `http.NewRequestWithContext()` for HTTP calls
- Check `ctx.Done()` before expensive operations
- Create child contexts with timeouts for sub-operations

### Testing
- Use table-driven tests with descriptive names
- Run with race detector: `go test -race ./...`
- Use subtests: `t.Run(tc.name, func(t *testing.T) {...})`
- Mock using interfaces, not dependency injection frameworks

### Logging
- Use `log/slog` (Go 1.21+) for structured logging
- Output JSON for production: `slog.NewJSONHandler()`
- Include correlation IDs in request logs
- Use appropriate levels: DEBUG for dev, INFO for ops, ERROR for failures

### HTTP Servers
- Always set timeouts: ReadTimeout, WriteTimeout, IdleTimeout
- Implement graceful shutdown with `srv.Shutdown(ctx)`
- Use middleware for cross-cutting concerns (logging, auth, recovery)
- Return proper error responses with consistent format

## LEARN Annotations
Throughout the codebase, you'll find `// LEARN:` comments that explain
why certain patterns are used. These map to concepts taught at top
universities (CMU 15-214, MIT 6.031, Stanford CS110).

## Phase Structure
- **Phase 1:** Foundations (interfaces, testing, error handling, HTTP patterns)
- **Phase 2:** Concurrency (goroutines, channels, worker pools)
- **Phase 3:** Generics (type parameters, constraints)
- **Phase 4:** Performance (profiling, benchmarks, memory pooling)
- **Phase 5:** Systems Programming (unsafe, mmap, CGO)
- **Phase 6:** Static Analysis (go/ast, custom linters)
- **Phase 7:** Distributed Systems (consensus, gRPC streaming)

## Commands
```bash
make build          # Build binary
make run            # Start server
make test           # Run tests with race detector
make lint           # Run linters
make learn          # Show current phase objectives
make exercise-1-1   # Start Exercise 1.1
make exercise-1-2   # Start Exercise 1.2
```

## Dependencies
- Go 1.22+
- Ollama (for local LLM inference)
- staticcheck (optional linter)

## Zuup Ecosystem Integration
GLR is designed to become a core component of the Zuup platform:
- **Veyra (Autonomy OS):** Tool execution layer with safety boundaries
- **Aureon (Procurement):** Audit logs for FAR/DFARS traceability
- **Civium (Halal Compliance):** Tool attestation hooks
- **zuup-core:** Shared primitives in `pkg/types`

## AI Assistant Guidelines
When helping with this codebase:
1. Preserve `// LEARN:` annotations - they're educational
2. Follow interface-based design patterns
3. Add table-driven tests for new functionality
4. Use structured logging with correlation IDs
5. Handle errors properly with context wrapping
6. Respect the phase structure for new features
7. Check exercises before adding similar functionality

